{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import pickle as pkl\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import tarfile\n",
    "import pickle\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(inputs, targets, batch_size, shuffle=True):\n",
    "    \"\"\"Divide a dataset (usually the training set) into mini-batches of a given size. This is a\n",
    "    'generator'\"\"\"\n",
    "    \n",
    "    if inputs.shape[0] % batch_size != 0:\n",
    "        raise RuntimeError('The number of data points must be a multiple of the batch size.')\n",
    "    num_batches = inputs.shape[0] // batch_size\n",
    "\n",
    "    if shuffle:\n",
    "        idxs = np.random.permutation(inputs.shape[0])\n",
    "        inputs = inputs[idxs, :]\n",
    "        targets = targets[idxs]\n",
    "\n",
    "    for m in range(num_batches):\n",
    "        yield inputs[m*batch_size:(m+1)*batch_size, :], \\\n",
    "              targets[m*batch_size:(m+1)*batch_size]       \n",
    "\n",
    "def split_train_test(inputs, targets, train_size, test_size, shuffle=True):\n",
    "    if shuffle:\n",
    "        idxs = np.random.permutation(inputs.shape[0])\n",
    "    else:\n",
    "        idxs = range(inputs.shape[0])\n",
    "    train_idxs = idxs[:train_size]\n",
    "    test_idxs = idxs[train_size: train_size+test_size]\n",
    "    print(f\"Split train and test, train size: {len(train_idxs)}, test_size: {len(test_idxs)}\")\n",
    "    return train_idxs, test_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decodeChinese(fpath, savePath = None):\n",
    "    with open(fpath, \"r\", encoding=\"gbk\") as f:\n",
    "        data = f.read()\n",
    "        if savePath: \n",
    "            with open(savePath, \"w\") as w:\n",
    "                w.write(data)\n",
    "    return data\n",
    "\n",
    "def to_var(tensor, cuda=False):\n",
    "    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n",
    "\n",
    "        Arguments:\n",
    "            tensor: A Tensor object.\n",
    "            cuda: A boolean flag indicating whether to use the GPU.\n",
    "\n",
    "        Returns:\n",
    "            A Variable object, on the GPU if cuda==True.\n",
    "    \"\"\"\n",
    "    if cuda:\n",
    "        return Variable(tensor.cuda())\n",
    "    else:\n",
    "        return Variable(tensor)\n",
    "\n",
    "def save_loss_plot(train_losses, val_losses, opts):\n",
    "    \"\"\"Saves a plot of the training and validation loss curves.\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.plot(range(len(train_losses)), train_losses)\n",
    "    plt.plot(range(len(val_losses)), val_losses)\n",
    "    plt.title('BS={}, nhid={}'.format(opts.batch_size, opts.hidden_size), fontsize=20)\n",
    "    plt.xlabel('Epochs', fontsize=16)\n",
    "    plt.ylabel('Loss', fontsize=16)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(opts.checkpoint_path, 'loss_plot.pdf'))\n",
    "    plt.close()\n",
    "\n",
    "def checkpoint(encoder, decoder, idx_dict, opts):\n",
    "    \"\"\"Saves the current encoder and decoder models, along with idx_dict, which\n",
    "    contains the char_to_index and index_to_char mappings, and the start_token\n",
    "    and end_token values.\n",
    "    \"\"\"\n",
    "    with open(os.path.join(opts.checkpoint_path, 'encoder.pt'), 'wb') as f:\n",
    "        torch.save(encoder, f)\n",
    "\n",
    "    with open(os.path.join(opts.checkpoint_path, 'decoder.pt'), 'wb') as f:\n",
    "        torch.save(decoder, f)\n",
    "\n",
    "    with open(os.path.join(opts.checkpoint_path, 'idx_dict.pkl'), 'wb') as f:\n",
    "        pkl.dump(idx_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus():\n",
    "    def __init__(self, corpus):\n",
    "        print(\"Initialize a corpus object\")\n",
    "        self.corpus = corpus\n",
    "        \n",
    "    def save(self, fpath):\n",
    "        with open(fpath, \"wb+\") as fd:\n",
    "            pickle.dump(self, fd)\n",
    "            \n",
    "    def preprocess(self, context_size, save=None):\n",
    "        sentences = self.corpus.split()\n",
    "        tokens = set()\n",
    "        num_of_data = 0\n",
    "        for s in sentences:\n",
    "            if len(s) + 2 > context_size:\n",
    "                num_of_data += len(s) + 2 - context_size\n",
    "                for char in s:\n",
    "                    tokens.add(char)\n",
    "        tokens.add(\"END\") #indicator for end of sentence\n",
    "        tokens.add(\"STA\") #indicator for start of sentence\n",
    "        self.vocal = sorted(list(tokens))\n",
    "        token_to_index = {char: index for (index, char) in enumerate(self.vocal)}\n",
    "        index_to_token = {token_to_index[char]: char for char in token_to_index}\n",
    "        vocSize = len(tokens)\n",
    "                \n",
    "        self.vocSize = vocSize\n",
    "        print(f\"Corpus has vocSize {self.vocSize}, including 'STA' and 'END' indicator\")\n",
    "        self.token_to_index = token_to_index\n",
    "        self.index_to_token = index_to_token\n",
    "        self.context_size = context_size\n",
    "        print(f\"Corpus has context size {self.context_size}\")\n",
    "        # init tensors\n",
    "        input_tensor = torch.zeros(num_of_data, context_size, vocSize)\n",
    "        output_tensor = torch.zeros(num_of_data, vocSize)\n",
    "\n",
    "        data_idx = 0\n",
    "        for s in sentences:\n",
    "            s = list(s)\n",
    "            s.insert(0, \"STA\")\n",
    "            s.append(\"END\")\n",
    "            if len(s) > context_size:\n",
    "                for j in range(len(s) - context_size - 1):\n",
    "                    for c in range(context_size):\n",
    "                        context_index = token_to_index[s[j+c]]\n",
    "                        input_tensor[data_idx][c][context_index] = 1\n",
    "                    output_index = token_to_index[s[j+c + 1]]\n",
    "                    output_tensor[data_idx][output_index] = 1\n",
    "                    data_idx += 1\n",
    "        data = {}\n",
    "        data[\"input_tensor\"] = input_tensor\n",
    "        data[\"output_tensor\"] = output_tensor\n",
    "        \n",
    "        print(f\"input_tensor shape: {input_tensor.shape}\")\n",
    "        print(f\"output_tensor shape: {output_tensor.shape}\")\n",
    "        self.data = data\n",
    "        \n",
    "        if save:\n",
    "            self.save(save)\n",
    "        \n",
    "    def tensor_to_word(self, tensor):\n",
    "        if len(tensor.shape) == 1:\n",
    "            if tensor.shape[0] != self.vocSize:\n",
    "                raise ValueError(f\"Bad tensor input. Should be either in shape(vocSize) or in shape(batch, vocSize), vocSize={self.vocSize}\")\n",
    "            index = int(torch.argmax(tensor))\n",
    "            return self.index_to_token[index] \n",
    "        if len(tensor.shape) == 2:\n",
    "            if tensor.shape[1] != self.vocSize:\n",
    "                raise ValueError(f\"Bad tensor input. Should be either in shape(vocSize) or in shape(batch, vocSize), vocSize={self.vocSize}\")\n",
    "            indexes = torch.argmax(tensor, dim=1)\n",
    "            text = ''.join([self.index_to_token[int(i)] for i in indexes])\n",
    "            return text\n",
    "        else:\n",
    "            raise ValueError(f\"Bad tensor input. Should be either in shape(vocSize) or in shape(batch, vocSize), vocSize={self.vocSize}\")\n",
    "     \n",
    "    def words_to_tensor(self, words):\n",
    "        # words is list\n",
    "        batch_size = len(words)\n",
    "        output_tensor = torch.zeros((batch_size, self.vocSize))\n",
    "        for i in range(batch_size):\n",
    "            w = words[i]\n",
    "            index = self.token_to_index[w]\n",
    "            output_tensor[i][index] = 1\n",
    "        return output_tensor\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocSize, embedding_dim, context_size, linear=False):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        self.vocSize = vocSize\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.context_size = context_size\n",
    "        self.linear = linear\n",
    "        # layers\n",
    "        if linear:\n",
    "            self.embedding_layer = nn.Linear(self.vocSize, self.embedding_dim)\n",
    "        else:\n",
    "            self.embedding_layer = nn.Embedding(self.vocSize, self.embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocSize)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # input batch x 3\n",
    "        batch_size = inputs.shape[0]\n",
    "        embeds = self.embedding_layer(inputs).view((batch_size, -1)) # batch x (context_size x embedding_dim)\n",
    "        o1 = F.sigmoid(self.linear1(embeds)) # batch x 128\n",
    "        o2 = self.linear2(o1) # batch x vocSize\n",
    "        o3 = self.softmax(o2) # batch x vocSize\n",
    "#         o3 = F.log_softmax(o2, dim=1)\n",
    "        return o3\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loss(test_input, test_output, model, loss_func, cuda, batch_size=100):\n",
    "    with torch.no_grad():\n",
    "        if cuda:\n",
    "            model.cuda()\n",
    "        loss_t = 0.0\n",
    "        for i, (input_b, out_b) in enumerate(get_batches(test_input, test_output, batch_size)):\n",
    "            input_b = to_var(input_b, cuda)\n",
    "            out_b = to_var(out_b, cuda)\n",
    "            output_predict = model(input_b)\n",
    "            loss_t += loss_func(output_predict, out_b).item()\n",
    "    return loss_t/(i+1)\n",
    "\n",
    "def show_next_word(languageModel, corpus, words, top = 5):\n",
    "    with torch.no_grad():\n",
    "        context_size = len(words)\n",
    "        if context_size != corpus.context_size:\n",
    "            raise ValueError(f\"Context size doesn't match need {corpus.context_size}\")\n",
    "        input_tensor = corpus.words_to_tensor(words).view(1, context_size, corpus.vocSize)\n",
    "        if not languageModel.linear:\n",
    "            input_tensor = torch.argmax(to_var(input_tensor), dim=2).type(torch.LongTensor)\n",
    "        else:\n",
    "            input_tensor = to_var(input_tensor)\n",
    "        output = languageModel(input_tensor)\n",
    "        idxs = torch.argsort(output, descending=True, dim=1)\n",
    "        print(f\"Top {top} candidates for {words}: \\n\")\n",
    "        for i in range(top):\n",
    "            print(f\"{corpus.index_to_token[int(idxs[0][i])]},       prob:{output[0][int(idxs[0][i])]}\")\n",
    "        return output\n",
    "\n",
    "def train(model, training, validation, opts):\n",
    "    '''\n",
    "    Return models of last training and of best validation loss \n",
    "    '''\n",
    "    training_input_tensor = training[\"input_tensor\"]\n",
    "    training_output_tensor = training[\"output_tensor\"]\n",
    "    training_batch_size = training[\"batch_size\"]\n",
    "    \n",
    "    validation_input_tensor = validation[\"input_tensor\"]\n",
    "    validation_output_tensor = validation[\"output_tensor\"]\n",
    "    validation_batch_size = validation[\"batch_size\"]\n",
    "    \n",
    "    loss_f =  opts.get(\"loss_function\", F.cross_entropy)\n",
    "    optimizer = opts.get(\"optimizer\", optim.Adam)\n",
    "    epoches = opts.get(\"epoches\", 10)\n",
    "    lr = opts.get(\"learning_rate\", 0.01)\n",
    "    loss_report = opts.get(\"loss_report\", 100)\n",
    "    cuda = opts.get(\"cuda\", False)\n",
    "    \n",
    "    optimizer = optimizer(model.parameters(), lr=lr)\n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "    \n",
    "    best_loss = None\n",
    "    best_param = None\n",
    "    for epoch in range(epoches):\n",
    "        print(f\"Epoch: {epoch + 1}\")\n",
    "        for i, (input_b, out_b) in enumerate(get_batches(training_input_tensor, training_output_tensor, training_batch_size)):\n",
    "            input_b = to_var(input_b, cuda)\n",
    "            out_b = to_var(out_b, cuda)\n",
    "            optimizer.zero_grad()   # zero the gradient buffers\n",
    "            output = model(input_b)\n",
    "            loss = loss_f(output, out_b)\n",
    "            loss.backward()\n",
    "            optimizer.step()    #\n",
    "            if (i+1)%loss_report == 0:\n",
    "                print(\"Batch:\", i+1, \"loss:\",loss.item())\n",
    "        # validation error\n",
    "        with torch.no_grad():\n",
    "            loss_t = validation_loss(validation_input_tensor, validation_output_tensor, model, loss_f, cuda, validation_batch_size)\n",
    "            if best_loss is None or loss_t < best_loss:\n",
    "                best_loss = loss_t\n",
    "                best_param = model.state_dict()\n",
    "            print(\"test loss:\",loss_t)\n",
    "    return (best_param, best_loss), model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = decodeChinese(\"./renjianshige.txt\", \"decoded.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize a corpus object\n"
     ]
    }
   ],
   "source": [
    "my_corpus = Corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus has vocSize 2362, including 'STA' and 'END' indicator\n",
      "Corpus has context size 3\n",
      "input_tensor shape: torch.Size([50514, 3, 2362])\n",
      "output_tensor shape: torch.Size([50514, 2362])\n"
     ]
    }
   ],
   "source": [
    "my_corpus.preprocess(3, \"corpus.pk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'生时代'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_corpus.tensor_to_word(my_corpus.data[\"input_tensor\"][102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LanguageModel(\n",
      "  (embedding_layer): Embedding(2362, 48)\n",
      "  (linear1): Linear(in_features=144, out_features=128, bias=True)\n",
      "  (linear2): Linear(in_features=128, out_features=2362, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n",
      "Split train and test, train size: 45500, test_size: 5000\n",
      "Epoch: 1\n",
      "Batch: 100 loss: 7.720731735229492\n",
      "Batch: 200 loss: 7.708575248718262\n",
      "Batch: 300 loss: 7.718446731567383\n",
      "Batch: 400 loss: 7.662111759185791\n",
      "test loss: 7.683767595291138\n",
      "Epoch: 2\n",
      "Batch: 100 loss: 7.718103408813477\n",
      "Batch: 200 loss: 7.677324295043945\n",
      "Batch: 300 loss: 7.679450511932373\n",
      "Batch: 400 loss: 7.713963031768799\n",
      "test loss: 7.681263380050659\n",
      "Epoch: 3\n",
      "Batch: 100 loss: 7.649297714233398\n",
      "Batch: 200 loss: 7.659726619720459\n",
      "Batch: 300 loss: 7.7179670333862305\n",
      "Batch: 400 loss: 7.698071479797363\n",
      "test loss: 7.680337448120117\n",
      "Epoch: 4\n",
      "Batch: 100 loss: 7.679096221923828\n",
      "Batch: 200 loss: 7.7183027267456055\n",
      "Batch: 300 loss: 7.666772365570068\n",
      "Batch: 400 loss: 7.674604415893555\n",
      "test loss: 7.678219842910766\n",
      "Epoch: 5\n",
      "Batch: 100 loss: 7.703049182891846\n",
      "Batch: 200 loss: 7.61562442779541\n",
      "Batch: 300 loss: 7.670083045959473\n",
      "Batch: 400 loss: 7.668611526489258\n",
      "test loss: 7.675172262191772\n",
      "Epoch: 6\n",
      "Batch: 100 loss: 7.668971061706543\n",
      "Batch: 200 loss: 7.650233268737793\n",
      "Batch: 300 loss: 7.662142753601074\n",
      "Batch: 400 loss: 7.728059768676758\n",
      "test loss: 7.674468297958374\n",
      "Epoch: 7\n",
      "Batch: 100 loss: 7.698451995849609\n",
      "Batch: 200 loss: 7.657313823699951\n",
      "Batch: 300 loss: 7.6686692237854\n",
      "Batch: 400 loss: 7.722574234008789\n",
      "test loss: 7.674158363342285\n",
      "Epoch: 8\n",
      "Batch: 100 loss: 7.628373622894287\n",
      "Batch: 200 loss: 7.628495693206787\n",
      "Batch: 300 loss: 7.657675266265869\n",
      "Batch: 400 loss: 7.652557849884033\n",
      "test loss: 7.674596090316772\n",
      "Epoch: 9\n",
      "Batch: 100 loss: 7.688076019287109\n",
      "Batch: 200 loss: 7.668188571929932\n",
      "Batch: 300 loss: 7.703649997711182\n",
      "Batch: 400 loss: 7.646533012390137\n",
      "test loss: 7.674406089782715\n",
      "Epoch: 10\n",
      "Batch: 100 loss: 7.678189277648926\n",
      "Batch: 200 loss: 7.6320013999938965\n",
      "Batch: 300 loss: 7.638442516326904\n",
      "Batch: 400 loss: 7.637783050537109\n",
      "test loss: 7.6742692470550535\n",
      "Epoch: 11\n",
      "Batch: 100 loss: 7.6511077880859375\n",
      "Batch: 200 loss: 7.608778476715088\n",
      "Batch: 300 loss: 7.6724629402160645\n",
      "Batch: 400 loss: 7.59830379486084\n",
      "test loss: 7.674140310287475\n",
      "Epoch: 12\n",
      "Batch: 100 loss: 7.628249645233154\n",
      "Batch: 200 loss: 7.727680206298828\n",
      "Batch: 300 loss: 7.648426532745361\n",
      "Batch: 400 loss: 7.658257961273193\n",
      "test loss: 7.674551086425781\n",
      "Epoch: 13\n",
      "Batch: 100 loss: 7.698744297027588\n",
      "Batch: 200 loss: 7.610438823699951\n",
      "Batch: 300 loss: 7.648013114929199\n",
      "Batch: 400 loss: 7.678339958190918\n",
      "test loss: 7.673948850631714\n",
      "Epoch: 14\n",
      "Batch: 100 loss: 7.6183929443359375\n",
      "Batch: 200 loss: 7.648007392883301\n",
      "Batch: 300 loss: 7.684060096740723\n",
      "Batch: 400 loss: 7.647231578826904\n",
      "test loss: 7.673970823287964\n",
      "Epoch: 15\n",
      "Batch: 100 loss: 7.667962551116943\n",
      "Batch: 200 loss: 7.65847635269165\n",
      "Batch: 300 loss: 7.638713836669922\n",
      "Batch: 400 loss: 7.6179986000061035\n",
      "test loss: 7.673836078643799\n",
      "Epoch: 16\n",
      "Batch: 100 loss: 7.652565956115723\n",
      "Batch: 200 loss: 7.6806640625\n",
      "Batch: 300 loss: 7.6684041023254395\n",
      "Batch: 400 loss: 7.688007354736328\n",
      "test loss: 7.673924856185913\n",
      "Epoch: 17\n",
      "Batch: 100 loss: 7.6283416748046875\n",
      "Batch: 200 loss: 7.638164520263672\n",
      "Batch: 300 loss: 7.664432525634766\n",
      "Batch: 400 loss: 7.6382341384887695\n",
      "test loss: 7.673871898651123\n",
      "Epoch: 18\n",
      "Batch: 100 loss: 7.687512397766113\n",
      "Batch: 200 loss: 7.65811824798584\n",
      "Batch: 300 loss: 7.6080193519592285\n",
      "Batch: 400 loss: 7.6280107498168945\n",
      "test loss: 7.674029474258423\n",
      "Epoch: 19\n",
      "Batch: 100 loss: 7.59799861907959\n",
      "Batch: 200 loss: 7.672513484954834\n",
      "Batch: 300 loss: 7.638043403625488\n",
      "Batch: 400 loss: 7.6980462074279785\n",
      "test loss: 7.673791799545288\n",
      "Epoch: 20\n",
      "Batch: 100 loss: 7.682250499725342\n",
      "Batch: 200 loss: 7.627764701843262\n",
      "Batch: 300 loss: 7.628025531768799\n",
      "Batch: 400 loss: 7.627975940704346\n",
      "test loss: 7.674062347412109\n",
      "Epoch: 21\n",
      "Batch: 100 loss: 7.648805618286133\n",
      "Batch: 200 loss: 7.648397445678711\n",
      "Batch: 300 loss: 7.647477626800537\n",
      "Batch: 400 loss: 7.727964401245117\n",
      "test loss: 7.674673671722412\n",
      "Epoch: 22\n",
      "Batch: 100 loss: 7.628875732421875\n",
      "Batch: 200 loss: 7.667989730834961\n",
      "Batch: 300 loss: 7.629134654998779\n",
      "Batch: 400 loss: 7.661027908325195\n",
      "test loss: 7.674941797256469\n",
      "Epoch: 23\n",
      "Batch: 100 loss: 7.568012714385986\n",
      "Batch: 200 loss: 7.648433208465576\n",
      "Batch: 300 loss: 7.663732528686523\n",
      "Batch: 400 loss: 7.657989501953125\n",
      "test loss: 7.6749680137634275\n",
      "Epoch: 24\n",
      "Batch: 100 loss: 7.628610134124756\n",
      "Batch: 200 loss: 7.6489996910095215\n",
      "Batch: 300 loss: 7.668245315551758\n",
      "Batch: 400 loss: 7.629061222076416\n",
      "test loss: 7.674691009521484\n",
      "Epoch: 25\n",
      "Batch: 100 loss: 7.62877082824707\n",
      "Batch: 200 loss: 7.62800407409668\n",
      "Batch: 300 loss: 7.658189296722412\n",
      "Batch: 400 loss: 7.667964458465576\n",
      "test loss: 7.67472767829895\n",
      "Epoch: 26\n",
      "Batch: 100 loss: 7.678028106689453\n",
      "Batch: 200 loss: 7.598297595977783\n",
      "Batch: 300 loss: 7.7078962326049805\n",
      "Batch: 400 loss: 7.6480326652526855\n",
      "test loss: 7.6750868225097655\n",
      "Epoch: 27\n",
      "Batch: 100 loss: 7.639971733093262\n",
      "Batch: 200 loss: 7.65841817855835\n",
      "Batch: 300 loss: 7.628175735473633\n",
      "Batch: 400 loss: 7.598357677459717\n",
      "test loss: 7.67511477470398\n",
      "Epoch: 28\n",
      "Batch: 100 loss: 7.66793966293335\n",
      "Batch: 200 loss: 7.6282429695129395\n",
      "Batch: 300 loss: 7.6480393409729\n",
      "Batch: 400 loss: 7.6829447746276855\n",
      "test loss: 7.675519533157349\n",
      "Epoch: 29\n",
      "Batch: 100 loss: 7.6679768562316895\n",
      "Batch: 200 loss: 7.6161274909973145\n",
      "Batch: 300 loss: 7.639324188232422\n",
      "Batch: 400 loss: 7.677987098693848\n",
      "test loss: 7.674852046966553\n",
      "Epoch: 30\n",
      "Batch: 100 loss: 7.608868598937988\n",
      "Batch: 200 loss: 7.568039417266846\n",
      "Batch: 300 loss: 7.62899923324585\n",
      "Batch: 400 loss: 7.648005962371826\n",
      "test loss: 7.675126829147339\n",
      "Epoch: 31\n",
      "Batch: 100 loss: 7.697250843048096\n",
      "Batch: 200 loss: 7.600766658782959\n",
      "Batch: 300 loss: 7.618043422698975\n",
      "Batch: 400 loss: 7.6783127784729\n",
      "test loss: 7.675375185012817\n",
      "Epoch: 32\n",
      "Batch: 100 loss: 7.66854190826416\n",
      "Batch: 200 loss: 7.5882158279418945\n",
      "Batch: 300 loss: 7.578101634979248\n",
      "Batch: 400 loss: 7.667203903198242\n",
      "test loss: 7.6749278163909915\n",
      "Epoch: 33\n",
      "Batch: 100 loss: 7.658006191253662\n",
      "Batch: 200 loss: 7.608977794647217\n",
      "Batch: 300 loss: 7.5970845222473145\n",
      "Batch: 400 loss: 7.638244152069092\n",
      "test loss: 7.675619449615478\n",
      "Epoch: 34\n",
      "Batch: 100 loss: 7.6377177238464355\n",
      "Batch: 200 loss: 7.598151683807373\n",
      "Batch: 300 loss: 7.651101589202881\n",
      "Batch: 400 loss: 7.661318302154541\n",
      "test loss: 7.676006555557251\n",
      "Epoch: 35\n",
      "Batch: 100 loss: 7.657989978790283\n",
      "Batch: 200 loss: 7.628055572509766\n",
      "Batch: 300 loss: 7.6015729904174805\n",
      "Batch: 400 loss: 7.651337146759033\n",
      "test loss: 7.6754079437255855\n",
      "Epoch: 36\n",
      "Batch: 100 loss: 7.667969226837158\n",
      "Batch: 200 loss: 7.618168354034424\n",
      "Batch: 300 loss: 7.5994086265563965\n",
      "Batch: 400 loss: 7.639145374298096\n",
      "test loss: 7.673146390914917\n",
      "Epoch: 37\n",
      "Batch: 100 loss: 7.628077507019043\n",
      "Batch: 200 loss: 7.598977565765381\n",
      "Batch: 300 loss: 7.630832672119141\n",
      "Batch: 400 loss: 7.638406276702881\n",
      "test loss: 7.671905565261841\n",
      "Epoch: 38\n",
      "Batch: 100 loss: 7.6186137199401855\n",
      "Batch: 200 loss: 7.633492946624756\n",
      "Batch: 300 loss: 7.6380839347839355\n",
      "Batch: 400 loss: 7.598259449005127\n",
      "test loss: 7.671432304382324\n",
      "Epoch: 39\n",
      "Batch: 100 loss: 7.638452053070068\n",
      "Batch: 200 loss: 7.619366645812988\n",
      "Batch: 300 loss: 7.638062000274658\n",
      "Batch: 400 loss: 7.648132801055908\n",
      "test loss: 7.671176147460938\n",
      "Epoch: 40\n",
      "Batch: 100 loss: 7.685726165771484\n",
      "Batch: 200 loss: 7.668111801147461\n",
      "Batch: 300 loss: 7.6482439041137695\n",
      "Batch: 400 loss: 7.697994232177734\n",
      "test loss: 7.670649013519287\n",
      "Epoch: 41\n",
      "Batch: 100 loss: 7.618427753448486\n",
      "Batch: 200 loss: 7.637802600860596\n",
      "Batch: 300 loss: 7.588031768798828\n",
      "Batch: 400 loss: 7.548654079437256\n",
      "test loss: 7.670598831176758\n",
      "Epoch: 42\n",
      "Batch: 100 loss: 7.65799617767334\n",
      "Batch: 200 loss: 7.6086039543151855\n",
      "Batch: 300 loss: 7.6477251052856445\n",
      "Batch: 400 loss: 7.639525890350342\n",
      "test loss: 7.670744142532349\n",
      "Epoch: 43\n",
      "Batch: 100 loss: 7.628182888031006\n",
      "Batch: 200 loss: 7.612832069396973\n",
      "Batch: 300 loss: 7.67796516418457\n",
      "Batch: 400 loss: 7.682955741882324\n",
      "test loss: 7.670393934249878\n",
      "Epoch: 44\n",
      "Batch: 100 loss: 7.578367233276367\n",
      "Batch: 200 loss: 7.610081195831299\n",
      "Batch: 300 loss: 7.6383056640625\n",
      "Batch: 400 loss: 7.618342876434326\n",
      "test loss: 7.670651435852051\n",
      "Epoch: 45\n",
      "Batch: 100 loss: 7.628524303436279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 200 loss: 7.648004055023193\n",
      "Batch: 300 loss: 7.632894515991211\n",
      "Batch: 400 loss: 7.657996654510498\n",
      "test loss: 7.670269241333008\n",
      "Epoch: 46\n",
      "Batch: 100 loss: 7.608029842376709\n",
      "Batch: 200 loss: 7.588305473327637\n",
      "Batch: 300 loss: 7.577999114990234\n",
      "Batch: 400 loss: 7.6479949951171875\n",
      "test loss: 7.670562448501587\n",
      "Epoch: 47\n",
      "Batch: 100 loss: 7.63812255859375\n",
      "Batch: 200 loss: 7.63809871673584\n",
      "Batch: 300 loss: 7.657971382141113\n",
      "Batch: 400 loss: 7.6072258949279785\n",
      "test loss: 7.670093250274658\n",
      "Epoch: 48\n",
      "Batch: 100 loss: 7.65797233581543\n",
      "Batch: 200 loss: 7.6183037757873535\n",
      "Batch: 300 loss: 7.648054599761963\n",
      "Batch: 400 loss: 7.6281657218933105\n",
      "test loss: 7.670127363204956\n",
      "Epoch: 49\n",
      "Batch: 100 loss: 7.647970199584961\n",
      "Batch: 200 loss: 7.647793769836426\n",
      "Batch: 300 loss: 7.6279473304748535\n",
      "Batch: 400 loss: 7.5690717697143555\n",
      "test loss: 7.669312181472779\n",
      "Epoch: 50\n",
      "Batch: 100 loss: 7.648627758026123\n",
      "Batch: 200 loss: 7.638344764709473\n",
      "Batch: 300 loss: 7.608561992645264\n",
      "Batch: 400 loss: 7.648199558258057\n",
      "test loss: 7.670105247497559\n"
     ]
    }
   ],
   "source": [
    "languageModel = LanguageModel(my_corpus.vocSize, 48, my_corpus.context_size)\n",
    "print(languageModel)\n",
    "\n",
    "train_idxs, validation_idxs = split_train_test(my_corpus.data[\"input_tensor\"], my_corpus.data[\"output_tensor\"], 45500, 5000)\n",
    "train_input = my_corpus.data[\"input_tensor\"][train_idxs]\n",
    "train_output = my_corpus.data[\"output_tensor\"][train_idxs]\n",
    "validation_input = my_corpus.data[\"input_tensor\"][validation_idxs]\n",
    "validation_output = my_corpus.data[\"output_tensor\"][validation_idxs]\n",
    "train_input = torch.argmax(train_input, dim=2).type(torch.LongTensor)\n",
    "train_output = torch.argmax(train_output, dim=1).type(torch.LongTensor)\n",
    "validation_input = torch.argmax(validation_input, dim=2).type(torch.LongTensor)\n",
    "validation_output = torch.argmax(validation_output, dim=1).type(torch.LongTensor)\n",
    "\n",
    "training = {\n",
    "    \"input_tensor\":train_input,\n",
    "    \"output_tensor\": train_output,\n",
    "    \"batch_size\": 100\n",
    "}\n",
    "\n",
    "validation = {\n",
    "    \"input_tensor\":validation_input,\n",
    "    \"output_tensor\": validation_output,\n",
    "    \"batch_size\": 100\n",
    "}\n",
    "\n",
    "opts = {\n",
    "    \"loss_function\": F.cross_entropy,\n",
    "    \"optimizer\": optim.Adam,\n",
    "    \"epoches\": 50,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"loss_report\": 100,\n",
    "    \"cuda\": False\n",
    "}\n",
    "\n",
    "(best_param, best_loss), languageModel = train(languageModel, training, validation, opts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 candidates for 我对你: \n",
      "\n",
      "的,       prob:0.9999771118164062\n",
      "不,       prob:1.8050252037937753e-05\n",
      "。,       prob:3.6775013541046064e-06\n",
      "是,       prob:1.2362371535346028e-06\n",
      "？,       prob:5.76659653361844e-09\n",
      "Top 5 candidates for 起已希: \n",
      "\n",
      "。,       prob:0.9261553883552551\n",
      "？,       prob:0.05110378563404083\n",
      "我,       prob:0.018049441277980804\n",
      "的,       prob:0.004071414470672607\n",
      "不,       prob:0.00038540392415598035\n",
      "Top 5 candidates for ['STA', '我', '有']: \n",
      "\n",
      "，,       prob:0.9998370409011841\n",
      "不,       prob:0.00014015873603057116\n",
      "。,       prob:2.229893289040774e-05\n",
      "的,       prob:4.910542088509828e-07\n",
      "是,       prob:9.66033741889305e-09\n",
      "Top 5 candidates for ['STA', '我', 'END']: \n",
      "\n",
      "，,       prob:0.9963118433952332\n",
      "是,       prob:0.003097912995144725\n",
      "不,       prob:0.0005893931956961751\n",
      "。,       prob:9.716601425679983e-07\n",
      "的,       prob:2.5536909392664953e-11\n",
      "Top 5 candidates for 你好吗: \n",
      "\n",
      "？,       prob:1.0\n",
      "。,       prob:2.8754064018698955e-08\n",
      "是,       prob:4.575181122845606e-09\n",
      "不,       prob:1.4009696180927733e-10\n",
      "的,       prob:6.324823043774019e-12\n",
      "Top 5 candidates for 就就就: \n",
      "\n",
      "是,       prob:0.9996379613876343\n",
      "不,       prob:0.0003617799375206232\n",
      "，,       prob:1.7634444304803765e-07\n",
      "。,       prob:2.020194322938096e-08\n",
      "的,       prob:8.515277372111996e-10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.0953e-13, 1.5217e-13, 2.4427e-13,  ..., 1.7105e-13, 1.7842e-13,\n",
       "         1.3332e-12]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_next_word(languageModel, my_corpus, '我对你')\n",
    "show_next_word(languageModel, my_corpus, '起已希')\n",
    "show_next_word(languageModel, my_corpus, ['STA', '我', '有'])\n",
    "show_next_word(languageModel, my_corpus, ['STA', '我', 'END'])\n",
    "show_next_word(languageModel, my_corpus, '你好吗')\n",
    "show_next_word(languageModel, my_corpus, '就就就')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split train and test, train size: 39500, test_size: 10000\n",
      "Batch: 100 loss: 7.688051223754883\n",
      "Batch: 200 loss: 7.678033828735352\n",
      "Batch: 300 loss: 7.7079877853393555\n",
      "test loss: 7.714195432662964\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "Batch: 100 loss: 7.7279953956604\n",
      "Batch: 200 loss: 7.707977294921875\n",
      "Batch: 300 loss: 7.757962703704834\n",
      "test loss: 7.709945483207703\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "Batch: 100 loss: 7.7180376052856445\n",
      "Batch: 200 loss: 7.712050914764404\n",
      "Batch: 300 loss: 7.670931339263916\n",
      "test loss: 7.69998601436615\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "Batch: 100 loss: 7.688957691192627\n",
      "Batch: 200 loss: 7.6944427490234375\n",
      "Batch: 300 loss: 7.688025951385498\n",
      "test loss: 7.675041689872741\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "Batch: 100 loss: 7.698200225830078\n",
      "Batch: 200 loss: 7.639062404632568\n",
      "Batch: 300 loss: 7.647737979888916\n",
      "test loss: 7.676521506309509\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "Batch: 100 loss: 7.658239364624023\n",
      "Batch: 200 loss: 7.648287773132324\n",
      "Batch: 300 loss: 7.6778693199157715\n",
      "test loss: 7.672319393157959\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "Batch: 100 loss: 7.738067150115967\n",
      "Batch: 200 loss: 7.68630838394165\n",
      "Batch: 300 loss: 7.687640190124512\n",
      "test loss: 7.6671305799484255\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "Batch: 100 loss: 7.647996425628662\n",
      "Batch: 200 loss: 7.708079814910889\n",
      "Batch: 300 loss: 7.648062705993652\n",
      "test loss: 7.668675265312195\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "Batch: 100 loss: 7.668041229248047\n",
      "Batch: 200 loss: 7.67800235748291\n",
      "Batch: 300 loss: 7.628073692321777\n",
      "test loss: 7.670395402908325\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "Batch: 100 loss: 7.698358535766602\n",
      "Batch: 200 loss: 7.658001899719238\n",
      "Batch: 300 loss: 7.687989711761475\n",
      "test loss: 7.6664239072799685\n"
     ]
    }
   ],
   "source": [
    "languageModel = LanguageModel(my_corpus.vocSize, 52, my_corpus.context_size, True)\n",
    "print(languageModel)\n",
    "loss_f = F.cross_entropy\n",
    "\n",
    "epoches = 10\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(languageModel.parameters(), lr=lr)\n",
    "loss_report = 100\n",
    "for epoch in range(epoches):\n",
    "    train_idxs, test_idxs = split_train_test(my_corpus.data[\"input_tensor\"], my_corpus.data[\"output_tensor\"], 39500, 10000)\n",
    "    train_input = my_corpus.data[\"input_tensor\"][train_idxs]\n",
    "    train_output = my_corpus.data[\"output_tensor\"][train_idxs]\n",
    "    test_input = my_corpus.data[\"input_tensor\"][test_idxs]\n",
    "    test_output = my_corpus.data[\"output_tensor\"][test_idxs]\n",
    "    for i, (input_b, out_b) in enumerate(get_batches(train_input, train_output, 100)):\n",
    "        input_b = to_var(input_b)\n",
    "        out_b = torch.argmax(to_var(out_b), dim=1).type(torch.LongTensor)\n",
    "        optimizer.zero_grad()   # zero the gradient buffers\n",
    "        output = languageModel(input_b)\n",
    "        loss = loss_f(output, out_b)\n",
    "        loss.backward()\n",
    "        optimizer.step()    #\n",
    "        if (i+1)%loss_report == 0:\n",
    "            print(\"Batch:\", i+1, \"loss:\",loss.item())\n",
    "    # validation error\n",
    "    with torch.no_grad():\n",
    "        loss_t = validation_loss(test_input, test_output, languageModel, loss_f, 100)\n",
    "#         if loss_t < lbest:\n",
    "#             lbest = loss_t\n",
    "#             bestp = mlp.state_dict()\n",
    "       \n",
    "        print(\"test loss:\",loss_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LanguageModel(\n",
      "  (embedding_layer): Embedding(250, 16)\n",
      "  (linear1): Linear(in_features=48, out_features=128, bias=True)\n",
      "  (linear2): Linear(in_features=128, out_features=250, bias=True)\n",
      ")\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1000 loss: 4.065740585327148\n",
      "Batch: 2000 loss: 3.846798896789551\n",
      "Batch: 3000 loss: 3.850691795349121\n",
      "test loss: 3.7236439556203864\n",
      "Epoch: 1\n",
      "Batch: 1000 loss: 3.903912305831909\n",
      "Batch: 2000 loss: 3.47682523727417\n",
      "Batch: 3000 loss: 3.4247121810913086\n",
      "test loss: 3.6603608218572474\n",
      "Epoch: 2\n",
      "Batch: 1000 loss: 3.6736810207366943\n",
      "Batch: 2000 loss: 3.6680121421813965\n",
      "Batch: 3000 loss: 4.133667945861816\n",
      "test loss: 3.6497012225530483\n",
      "Epoch: 3\n",
      "Batch: 1000 loss: 3.8373239040374756\n",
      "Batch: 2000 loss: 3.752830743789673\n",
      "Batch: 3000 loss: 3.7197813987731934\n",
      "test loss: 3.657488513249223\n",
      "Epoch: 4\n",
      "Batch: 1000 loss: 3.8906915187835693\n",
      "Batch: 2000 loss: 3.8080813884735107\n",
      "Batch: 3000 loss: 3.625755548477173\n",
      "test loss: 3.6180806816265147\n",
      "Epoch: 5\n",
      "Batch: 1000 loss: 3.8452494144439697\n",
      "Batch: 2000 loss: 3.647465705871582\n",
      "Batch: 3000 loss: 3.6182546615600586\n",
      "test loss: 3.635104233731506\n",
      "Epoch: 6\n",
      "Batch: 1000 loss: 4.3388590812683105\n",
      "Batch: 2000 loss: 3.405104637145996\n",
      "Batch: 3000 loss: 3.182595729827881\n",
      "test loss: 3.6024559749070035\n",
      "Epoch: 7\n",
      "Batch: 1000 loss: 3.7417919635772705\n",
      "Batch: 2000 loss: 3.187980890274048\n",
      "Batch: 3000 loss: 3.1015536785125732\n",
      "test loss: 3.6129184861336987\n",
      "Epoch: 8\n",
      "Batch: 1000 loss: 3.666123151779175\n",
      "Batch: 2000 loss: 3.4258103370666504\n",
      "Batch: 3000 loss: 3.588360548019409\n",
      "test loss: 3.5790826202720725\n",
      "Epoch: 9\n",
      "Batch: 1000 loss: 3.6255276203155518\n",
      "Batch: 2000 loss: 3.351898193359375\n",
      "Batch: 3000 loss: 3.5600388050079346\n",
      "test loss: 3.6093211025320073\n",
      "Epoch: 10\n",
      "Batch: 1000 loss: 3.7061057090759277\n",
      "Batch: 2000 loss: 3.8525619506835938\n",
      "Batch: 3000 loss: 3.600338935852051\n",
      "test loss: 3.6233076926200622\n",
      "Epoch: 11\n",
      "Batch: 1000 loss: 3.992419719696045\n",
      "Batch: 2000 loss: 3.353231191635132\n",
      "Batch: 3000 loss: 3.5695509910583496\n",
      "test loss: 3.670554805058305\n",
      "Epoch: 12\n",
      "Batch: 1000 loss: 3.7210888862609863\n",
      "Batch: 2000 loss: 3.7555575370788574\n",
      "Batch: 3000 loss: 3.659153699874878\n",
      "test loss: 3.64836775102923\n",
      "Epoch: 13\n",
      "Batch: 1000 loss: 3.846221923828125\n",
      "Batch: 2000 loss: 3.519188642501831\n",
      "Batch: 3000 loss: 3.4073801040649414\n",
      "test loss: 3.5952599653633692\n",
      "Epoch: 14\n",
      "Batch: 1000 loss: 3.5154662132263184\n",
      "Batch: 2000 loss: 3.3805654048919678\n",
      "Batch: 3000 loss: 3.671123743057251\n",
      "test loss: 3.6172885556374825\n",
      "Epoch: 15\n",
      "Batch: 1000 loss: 3.9611928462982178\n",
      "Batch: 2000 loss: 3.7585790157318115\n",
      "Batch: 3000 loss: 3.5829849243164062\n",
      "test loss: 3.597662709861673\n",
      "Epoch: 16\n",
      "Batch: 1000 loss: 3.7382495403289795\n",
      "Batch: 2000 loss: 3.422786235809326\n",
      "Batch: 3000 loss: 3.589109182357788\n",
      "test loss: 3.6057081689116774\n",
      "Epoch: 17\n",
      "Batch: 1000 loss: 3.870035409927368\n",
      "Batch: 2000 loss: 3.214216709136963\n",
      "Batch: 3000 loss: 3.560372829437256\n",
      "test loss: 3.6184746911448817\n",
      "Epoch: 18\n",
      "Batch: 1000 loss: 3.4339075088500977\n",
      "Batch: 2000 loss: 3.4348304271698\n",
      "Batch: 3000 loss: 3.3965959548950195\n",
      "test loss: 3.5794026103070986\n",
      "Epoch: 19\n",
      "Batch: 1000 loss: 3.4026377201080322\n",
      "Batch: 2000 loss: 3.6425671577453613\n",
      "Batch: 3000 loss: 3.6643800735473633\n",
      "test loss: 3.601097762712868\n",
      "Epoch: 20\n",
      "Batch: 1000 loss: 3.449570894241333\n",
      "Batch: 2000 loss: 3.583057165145874\n",
      "Batch: 3000 loss: 3.5611112117767334\n",
      "test loss: 3.6282460807472146\n",
      "Epoch: 21\n",
      "Batch: 1000 loss: 3.3511290550231934\n",
      "Batch: 2000 loss: 3.4993479251861572\n",
      "Batch: 3000 loss: 3.6807661056518555\n",
      "test loss: 3.587147842940464\n",
      "Epoch: 22\n",
      "Batch: 1000 loss: 3.5887274742126465\n",
      "Batch: 2000 loss: 3.4415242671966553\n",
      "Batch: 3000 loss: 3.767784833908081\n",
      "test loss: 3.564838149470668\n",
      "Epoch: 23\n",
      "Batch: 1000 loss: 3.549325942993164\n",
      "Batch: 2000 loss: 4.039292335510254\n",
      "Batch: 3000 loss: 3.565152168273926\n",
      "test loss: 3.5872672783431185\n",
      "Epoch: 24\n",
      "Batch: 1000 loss: 3.47636342048645\n",
      "Batch: 2000 loss: 3.7563412189483643\n",
      "Batch: 3000 loss: 3.8957009315490723\n",
      "test loss: 3.618213334647558\n",
      "Epoch: 25\n",
      "Batch: 1000 loss: 3.9325132369995117\n",
      "Batch: 2000 loss: 3.4881153106689453\n",
      "Batch: 3000 loss: 3.6827874183654785\n",
      "test loss: 3.6606888755675286\n",
      "Epoch: 26\n",
      "Batch: 1000 loss: 3.4708070755004883\n",
      "Batch: 2000 loss: 3.5731353759765625\n",
      "Batch: 3000 loss: 3.6102545261383057\n",
      "test loss: 3.6029787919854606\n",
      "Epoch: 27\n",
      "Batch: 1000 loss: 3.790195941925049\n",
      "Batch: 2000 loss: 3.393815517425537\n",
      "Batch: 3000 loss: 3.4211244583129883\n",
      "test loss: 3.602841156272478\n",
      "Epoch: 28\n",
      "Batch: 1000 loss: 3.865229845046997\n",
      "Batch: 2000 loss: 3.119126796722412\n",
      "Batch: 3000 loss: 3.727229356765747\n",
      "test loss: 3.5896685723335513\n",
      "Epoch: 29\n",
      "Batch: 1000 loss: 3.8913779258728027\n",
      "Batch: 2000 loss: 3.604257583618164\n",
      "Batch: 3000 loss: 3.463502883911133\n",
      "test loss: 3.5632526869414956\n",
      "Epoch: 30\n",
      "Batch: 1000 loss: 3.698598861694336\n",
      "Batch: 2000 loss: 3.5680792331695557\n",
      "Batch: 3000 loss: 3.7214980125427246\n",
      "test loss: 3.5698060117742068\n",
      "Epoch: 31\n",
      "Batch: 1000 loss: 3.7888050079345703\n",
      "Batch: 2000 loss: 3.7130258083343506\n",
      "Batch: 3000 loss: 3.839595079421997\n",
      "test loss: 3.578855786785003\n",
      "Epoch: 32\n",
      "Batch: 1000 loss: 3.488978862762451\n",
      "Batch: 2000 loss: 3.594935894012451\n",
      "Batch: 3000 loss: 3.708031415939331\n",
      "test loss: 3.588031961584604\n",
      "Epoch: 33\n",
      "Batch: 1000 loss: 3.6138482093811035\n",
      "Batch: 2000 loss: 3.4832401275634766\n",
      "Batch: 3000 loss: 3.4285285472869873\n",
      "test loss: 3.5598959466462494\n",
      "Epoch: 34\n",
      "Batch: 1000 loss: 3.506864070892334\n",
      "Batch: 2000 loss: 3.4927773475646973\n",
      "Batch: 3000 loss: 3.423630952835083\n",
      "test loss: 3.5666622192628923\n",
      "Epoch: 35\n",
      "Batch: 1000 loss: 3.2022430896759033\n",
      "Batch: 2000 loss: 3.7990732192993164\n",
      "Batch: 3000 loss: 3.5735390186309814\n",
      "test loss: 3.539171587523594\n",
      "Epoch: 36\n",
      "Batch: 1000 loss: 4.153641223907471\n",
      "Batch: 2000 loss: 3.8951663970947266\n",
      "Batch: 3000 loss: 3.4419116973876953\n",
      "test loss: 3.565837634507046\n",
      "Epoch: 37\n",
      "Batch: 1000 loss: 3.2946746349334717\n",
      "Batch: 2000 loss: 3.817521333694458\n",
      "Batch: 3000 loss: 3.4119327068328857\n",
      "test loss: 3.5453928080938195\n",
      "Epoch: 38\n",
      "Batch: 1000 loss: 3.6272096633911133\n",
      "Batch: 2000 loss: 3.5115745067596436\n",
      "Batch: 3000 loss: 3.792588710784912\n",
      "test loss: 3.560462943969234\n",
      "Epoch: 39\n",
      "Batch: 1000 loss: 3.4526455402374268\n",
      "Batch: 2000 loss: 3.858971357345581\n",
      "Batch: 3000 loss: 3.1411612033843994\n",
      "test loss: 3.52126612406905\n",
      "Epoch: 40\n",
      "Batch: 1000 loss: 3.5632550716400146\n",
      "Batch: 2000 loss: 3.850919246673584\n",
      "Batch: 3000 loss: 3.365598678588867\n",
      "test loss: 3.5346718029309345\n",
      "Epoch: 41\n",
      "Batch: 1000 loss: 3.472949743270874\n",
      "Batch: 2000 loss: 3.631631374359131\n",
      "Batch: 3000 loss: 3.4807162284851074\n",
      "test loss: 3.569208151807067\n",
      "Epoch: 42\n",
      "Batch: 1000 loss: 3.4453115463256836\n",
      "Batch: 2000 loss: 3.6967110633850098\n",
      "Batch: 3000 loss: 3.463876724243164\n",
      "test loss: 3.556135404750865\n",
      "Epoch: 43\n",
      "Batch: 1000 loss: 3.5927090644836426\n",
      "Batch: 2000 loss: 3.7645769119262695\n",
      "Batch: 3000 loss: 3.7803902626037598\n",
      "test loss: 3.576517263022802\n",
      "Epoch: 44\n",
      "Batch: 1000 loss: 3.60549259185791\n",
      "Batch: 2000 loss: 3.4838075637817383\n",
      "Batch: 3000 loss: 3.2023532390594482\n",
      "test loss: 3.5862363415379677\n",
      "Epoch: 45\n",
      "Batch: 1000 loss: 3.8019537925720215\n",
      "Batch: 2000 loss: 3.2621397972106934\n",
      "Batch: 3000 loss: 3.9665687084198\n",
      "test loss: 3.573281298401535\n",
      "Epoch: 46\n",
      "Batch: 1000 loss: 3.6124203205108643\n",
      "Batch: 2000 loss: 3.415299654006958\n",
      "Batch: 3000 loss: 3.375441312789917\n",
      "test loss: 3.578142026675645\n",
      "Epoch: 47\n",
      "Batch: 1000 loss: 3.4783315658569336\n",
      "Batch: 2000 loss: 3.0645322799682617\n",
      "Batch: 3000 loss: 3.9044313430786133\n",
      "test loss: 3.5873607102260796\n",
      "Epoch: 48\n",
      "Batch: 1000 loss: 3.747060775756836\n",
      "Batch: 2000 loss: 3.3230276107788086\n",
      "Batch: 3000 loss: 3.8583879470825195\n",
      "test loss: 3.570362819138394\n",
      "Epoch: 49\n",
      "Batch: 1000 loss: 3.8055996894836426\n",
      "Batch: 2000 loss: 3.720255136489868\n",
      "Batch: 3000 loss: 3.0969924926757812\n",
      "test loss: 3.570139031769127\n"
     ]
    }
   ],
   "source": [
    "def test_validation_loss(test_input, test_output, model, loss_func, batch_size=100):\n",
    "    with torch.no_grad():\n",
    "        loss_t = 0.0\n",
    "        for i, (input_b, out_b) in enumerate(get_batches(test_input, test_output, batch_size)):\n",
    "            if model.linear:\n",
    "                input_b = to_var(input_b)\n",
    "            output_predict = model(input_b)\n",
    "            loss_t += loss_func(output_predict, out_b).item()\n",
    "    return loss_t/(i+1)\n",
    "languageModel = LanguageModel(250, 16, 3, False)\n",
    "print(languageModel)\n",
    "loss_f = F.cross_entropy\n",
    "epoches = 50\n",
    "lr = 0.1\n",
    "optimizer = optim.Adam(languageModel.parameters(), lr=lr)\n",
    "loss_report = 1000\n",
    "for epoch in range(epoches):\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    train_input = torch.tensor(train_inputs).type(torch.LongTensor)\n",
    "    train_output = torch.tensor(train_targets).type(torch.LongTensor)\n",
    "    test_input = torch.tensor(valid_inputs).type(torch.LongTensor)\n",
    "    test_output = torch.tensor(valid_targets).type(torch.LongTensor)\n",
    "    for i, (input_b, out_b) in enumerate(get_batches(train_input, train_output, 100)):\n",
    "#         input_b = torch.argmax(to_var(input_b), dim=2).type(torch.LongTensor)\n",
    "#         out_b = torch.argmax(to_var(out_b), dim=1).type(torch.LongTensor)\n",
    "        optimizer.zero_grad()   # zero the gradient buffers\n",
    "        output = languageModel(input_b)\n",
    "        loss = loss_f(output, out_b)\n",
    "        loss.backward()\n",
    "        optimizer.step()    #\n",
    "        if (i+1)%loss_report == 0:\n",
    "            print(\"Batch:\", i+1, \"loss:\",loss.item())\n",
    "    # validation error\n",
    "    with torch.no_grad():\n",
    "        loss_t = test_validation_loss(test_input, test_output, languageModel, loss_f, 100)\n",
    "#         if loss_t < lbest:\n",
    "#             lbest = loss_t\n",
    "#             bestp = mlp.state_dict()\n",
    "       \n",
    "        print(\"test loss:\",loss_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
