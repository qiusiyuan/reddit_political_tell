{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import pickle as pkl\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import tarfile\n",
    "import pickle\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(inputs, targets, batch_size, shuffle=True):\n",
    "    \"\"\"Divide a dataset (usually the training set) into mini-batches of a given size. This is a\n",
    "    'generator'\"\"\"\n",
    "    \n",
    "    if inputs.shape[0] % batch_size != 0:\n",
    "        raise RuntimeError('The number of data points must be a multiple of the batch size.')\n",
    "    num_batches = inputs.shape[0] // batch_size\n",
    "\n",
    "    if shuffle:\n",
    "        idxs = np.random.permutation(inputs.shape[0])\n",
    "        inputs = inputs[idxs, :]\n",
    "        targets = targets[idxs]\n",
    "\n",
    "    for m in range(num_batches):\n",
    "        yield inputs[m*batch_size:(m+1)*batch_size, :], \\\n",
    "              targets[m*batch_size:(m+1)*batch_size]       \n",
    "\n",
    "def split_train_test(inputs, targets, train_size, test_size, shuffle=True):\n",
    "    if shuffle:\n",
    "        idxs = np.random.permutation(inputs.shape[0])\n",
    "    else:\n",
    "        idxs = range(inputs.shape[0])\n",
    "    train_idxs = idxs[:train_size]\n",
    "    test_idxs = idxs[train_size: train_size+test_size]\n",
    "    print(f\"Split train and test, train size: {len(train_idxs)}, test_size: {len(test_idxs)}\")\n",
    "    return train_idxs, test_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decodeChinese(fpath, savePath = None):\n",
    "    with open(fpath, \"r\", encoding=\"gbk\") as f:\n",
    "        data = f.read()\n",
    "        if savePath: \n",
    "            with open(savePath, \"w\") as w:\n",
    "                w.write(data)\n",
    "    return data\n",
    "\n",
    "def to_var(tensor, cuda=False):\n",
    "    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n",
    "\n",
    "        Arguments:\n",
    "            tensor: A Tensor object.\n",
    "            cuda: A boolean flag indicating whether to use the GPU.\n",
    "\n",
    "        Returns:\n",
    "            A Variable object, on the GPU if cuda==True.\n",
    "    \"\"\"\n",
    "    if cuda:\n",
    "        return Variable(tensor.cuda())\n",
    "    else:\n",
    "        return Variable(tensor)\n",
    "\n",
    "def save_loss_plot(train_losses, val_losses, opts):\n",
    "    \"\"\"Saves a plot of the training and validation loss curves.\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.plot(range(len(train_losses)), train_losses)\n",
    "    plt.plot(range(len(val_losses)), val_losses)\n",
    "    plt.title('BS={}, nhid={}'.format(opts.batch_size, opts.hidden_size), fontsize=20)\n",
    "    plt.xlabel('Epochs', fontsize=16)\n",
    "    plt.ylabel('Loss', fontsize=16)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(opts.checkpoint_path, 'loss_plot.pdf'))\n",
    "    plt.close()\n",
    "\n",
    "def checkpoint(encoder, decoder, idx_dict, opts):\n",
    "    \"\"\"Saves the current encoder and decoder models, along with idx_dict, which\n",
    "    contains the char_to_index and index_to_char mappings, and the start_token\n",
    "    and end_token values.\n",
    "    \"\"\"\n",
    "    with open(os.path.join(opts.checkpoint_path, 'encoder.pt'), 'wb') as f:\n",
    "        torch.save(encoder, f)\n",
    "\n",
    "    with open(os.path.join(opts.checkpoint_path, 'decoder.pt'), 'wb') as f:\n",
    "        torch.save(decoder, f)\n",
    "\n",
    "    with open(os.path.join(opts.checkpoint_path, 'idx_dict.pkl'), 'wb') as f:\n",
    "        pkl.dump(idx_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Corpus():\n",
    "    def __init__(self, corpus):\n",
    "        print(\"Initialize a corpus object\")\n",
    "        self.corpus = corpus\n",
    "        \n",
    "    def save(self, fpath):\n",
    "        with open(fpath, \"wb+\") as fd:\n",
    "            pickle.dump(self, fd)\n",
    "            \n",
    "    def preprocess(self, context_size, save=None):\n",
    "        sentences = self.corpus.split()\n",
    "        tokens = set()\n",
    "        num_of_data = 0\n",
    "        for s in sentences:\n",
    "            if len(s) + 2 > context_size:\n",
    "                num_of_data += len(s) + 2 - context_size\n",
    "                for char in s:\n",
    "                    tokens.add(char)\n",
    "        tokens.add(\"END\") #indicator for end of sentence\n",
    "        tokens.add(\"STA\") #indicator for start of sentence\n",
    "        self.vocal = sorted(list(tokens))\n",
    "        token_to_index = {char: index for (index, char) in enumerate(self.vocal)}\n",
    "        index_to_token = {token_to_index[char]: char for char in token_to_index}\n",
    "        vocSize = len(tokens)\n",
    "                \n",
    "        self.vocSize = vocSize\n",
    "        print(f\"Corpus has vocSize {self.vocSize}, including 'STA' and 'END' indicator\")\n",
    "        self.token_to_index = token_to_index\n",
    "        self.index_to_token = index_to_token\n",
    "        self.context_size = context_size\n",
    "        print(f\"Corpus has context size {self.context_size}\")\n",
    "        # init tensors\n",
    "        input_tensor = torch.zeros(num_of_data, context_size, vocSize)\n",
    "        output_tensor = torch.zeros(num_of_data, vocSize)\n",
    "\n",
    "        data_idx = 0\n",
    "        for s in sentences:\n",
    "            s = list(s)\n",
    "            s.insert(0, \"STA\")\n",
    "            s.append(\"END\")\n",
    "            if len(s) > context_size:\n",
    "                for j in range(len(s) - context_size - 1):\n",
    "                    for c in range(context_size):\n",
    "                        context_index = token_to_index[s[j+c]]\n",
    "                        input_tensor[data_idx][c][context_index] = 1\n",
    "                    output_index = token_to_index[s[j+c + 1]]\n",
    "                    output_tensor[data_idx][output_index] = 1\n",
    "                    data_idx += 1\n",
    "        data = {}\n",
    "        data[\"input_tensor\"] = input_tensor\n",
    "        data[\"output_tensor\"] = output_tensor\n",
    "\n",
    "        self.data = data\n",
    "\n",
    "        if save:\n",
    "            self.save(save)\n",
    "        \n",
    "    def tensor_to_word(self, tensor):\n",
    "        if len(tensor.shape) == 1:\n",
    "            if tensor.shape[0] != self.vocSize:\n",
    "                raise ValueError(f\"Bad tensor input. Should be either in shape(vocSize) or in shape(batch, vocSize), vocSize={self.vocSize}\")\n",
    "            index = int(torch.argmax(tensor))\n",
    "            return self.index_to_token[index] \n",
    "        if len(tensor.shape) == 2:\n",
    "            if tensor.shape[1] != self.vocSize:\n",
    "                raise ValueError(f\"Bad tensor input. Should be either in shape(vocSize) or in shape(batch, vocSize), vocSize={self.vocSize}\")\n",
    "            indexes = torch.argmax(tensor, dim=1)\n",
    "            text = ''.join([self.index_to_token[int(i)] for i in indexes])\n",
    "            return text\n",
    "        else:\n",
    "            raise ValueError(f\"Bad tensor input. Should be either in shape(vocSize) or in shape(batch, vocSize), vocSize={self.vocSize}\")\n",
    "     \n",
    "    def words_to_tensor(self, words):\n",
    "        # words is list\n",
    "        batch_size = len(words)\n",
    "        output_tensor = torch.zeros((batch_size, self.vocSize))\n",
    "        for i in range(batch_size):\n",
    "            w = words[i]\n",
    "            index = self.token_to_index[w]\n",
    "            output_tensor[i][index] = 1\n",
    "        return output_tensor\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loss(test_input, test_output, model, loss_func, batch_size=100):\n",
    "    with torch.no_grad():\n",
    "        loss_t = 0.0\n",
    "        for i, (input_b, out_b) in enumerate(get_batches(test_input, test_output, batch_size)):\n",
    "            if model.linear:\n",
    "                input_b = to_var(input_b)\n",
    "            else:\n",
    "                input_b = torch.argmax(to_var(input_b), dim=2).type(torch.LongTensor)\n",
    "            out_b = torch.argmax(to_var(out_b), dim=1).type(torch.LongTensor)\n",
    "            output_predict = model(input_b)\n",
    "            loss_t += loss_func(output_predict, out_b).item()\n",
    "    return loss_t/(i+1)\n",
    "\n",
    "def show_next_word(languageModel, corpus, words, top = 5):\n",
    "    with torch.no_grad():\n",
    "        context_size = len(words)\n",
    "        if context_size != corpus.context_size:\n",
    "            raise ValueError(f\"Context size doesn't match need {corpus.context_size}\")\n",
    "        input_tensor = corpus.words_to_tensor(words).view(1, context_size, corpus.vocSize)\n",
    "        if not languageModel.linear:\n",
    "            input_tensor = torch.argmax(to_var(input_tensor), dim=2).type(torch.LongTensor)\n",
    "        else:\n",
    "            input_tensor = to_var(input_tensor)\n",
    "        output = languageModel(input_tensor)\n",
    "        idxs = torch.argsort(output, descending=True, dim=1)\n",
    "        print(f\"Top {top} candidates for {words}: \\n\")\n",
    "        for i in range(top):\n",
    "            print(f\"{corpus.index_to_token[int(idxs[0][i])]},       prob:{output[0][int(idxs[0][i])]}\")\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocSize, embedding_dim, context_size, linear=True):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        self.vocSize = vocSize\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.context_size = context_size\n",
    "        self.linear = linear\n",
    "        # layers\n",
    "        if linear:\n",
    "            self.embedding_layer = nn.Linear(self.vocSize, self.embedding_dim)\n",
    "        else:\n",
    "            self.embedding_layer = nn.Embedding(self.vocSize, self.embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocSize)\n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # input batch x 3\n",
    "        batch_size = inputs.shape[0]\n",
    "        embeds = self.embedding_layer(inputs).view((batch_size, -1)) # batch x (context_size x embedding_dim)\n",
    "        o1 = F.sigmoid(self.linear1(embeds)) # batch x 128\n",
    "        o2 = self.linear2(o1) # batch x vocSize\n",
    "#         o3 = self.softmax(o2) # batch x vocSize\n",
    "        o3 = F.log_softmax(o2, dim=1)\n",
    "        return o3\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = decodeChinese(\"./renjianshige.txt\", \"decoded.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize a corpus object\n"
     ]
    }
   ],
   "source": [
    "my_corpus = Corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus has vocSize 2362, including 'STA' and 'END' indicator\n",
      "Corpus has context size 4\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-27fe0506c3f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_corpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"corpus.pk\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-436b010d4dbd>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(self, context_size, save)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtensor_to_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-436b010d4dbd>\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fpath)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb+\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 22] Invalid argument"
     ]
    }
   ],
   "source": [
    "my_corpus.preprocess(4, \"corpus.pk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'起已希望'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_corpus.tensor_to_word(my_corpus.data[\"input_tensor\"][102])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split train and test, train size: 39500, test_size: 10000\n",
      "Batch: 100 loss: 7.688051223754883\n",
      "Batch: 200 loss: 7.678033828735352\n",
      "Batch: 300 loss: 7.7079877853393555\n",
      "test loss: 7.714195432662964\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "Batch: 100 loss: 7.7279953956604\n",
      "Batch: 200 loss: 7.707977294921875\n",
      "Batch: 300 loss: 7.757962703704834\n",
      "test loss: 7.709945483207703\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "Batch: 100 loss: 7.7180376052856445\n",
      "Batch: 200 loss: 7.712050914764404\n",
      "Batch: 300 loss: 7.670931339263916\n",
      "test loss: 7.69998601436615\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "Batch: 100 loss: 7.688957691192627\n",
      "Batch: 200 loss: 7.6944427490234375\n",
      "Batch: 300 loss: 7.688025951385498\n",
      "test loss: 7.675041689872741\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "Batch: 100 loss: 7.698200225830078\n",
      "Batch: 200 loss: 7.639062404632568\n",
      "Batch: 300 loss: 7.647737979888916\n",
      "test loss: 7.676521506309509\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "Batch: 100 loss: 7.658239364624023\n",
      "Batch: 200 loss: 7.648287773132324\n",
      "Batch: 300 loss: 7.6778693199157715\n",
      "test loss: 7.672319393157959\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "Batch: 100 loss: 7.738067150115967\n",
      "Batch: 200 loss: 7.68630838394165\n",
      "Batch: 300 loss: 7.687640190124512\n",
      "test loss: 7.6671305799484255\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "Batch: 100 loss: 7.647996425628662\n",
      "Batch: 200 loss: 7.708079814910889\n",
      "Batch: 300 loss: 7.648062705993652\n",
      "test loss: 7.668675265312195\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "Batch: 100 loss: 7.668041229248047\n",
      "Batch: 200 loss: 7.67800235748291\n",
      "Batch: 300 loss: 7.628073692321777\n",
      "test loss: 7.670395402908325\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "Batch: 100 loss: 7.698358535766602\n",
      "Batch: 200 loss: 7.658001899719238\n",
      "Batch: 300 loss: 7.687989711761475\n",
      "test loss: 7.6664239072799685\n"
     ]
    }
   ],
   "source": [
    "languageModel = LanguageModel(my_corpus.vocSize, 52, my_corpus.context_size, True)\n",
    "print(languageModel)\n",
    "loss_f = F.cross_entropy\n",
    "\n",
    "epoches = 10\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(languageModel.parameters(), lr=lr)\n",
    "loss_report = 100\n",
    "for epoch in range(epoches):\n",
    "    train_idxs, test_idxs = split_train_test(my_corpus.data[\"input_tensor\"], my_corpus.data[\"output_tensor\"], 39500, 10000)\n",
    "    train_input = my_corpus.data[\"input_tensor\"][train_idxs]\n",
    "    train_output = my_corpus.data[\"output_tensor\"][train_idxs]\n",
    "    test_input = my_corpus.data[\"input_tensor\"][test_idxs]\n",
    "    test_output = my_corpus.data[\"output_tensor\"][test_idxs]\n",
    "    for i, (input_b, out_b) in enumerate(get_batches(train_input, train_output, 100)):\n",
    "        input_b = to_var(input_b)\n",
    "        out_b = torch.argmax(to_var(out_b), dim=1).type(torch.LongTensor)\n",
    "        optimizer.zero_grad()   # zero the gradient buffers\n",
    "        output = languageModel(input_b)\n",
    "        loss = loss_f(output, out_b)\n",
    "        loss.backward()\n",
    "        optimizer.step()    #\n",
    "        if (i+1)%loss_report == 0:\n",
    "            print(\"Batch:\", i+1, \"loss:\",loss.item())\n",
    "    # validation error\n",
    "    with torch.no_grad():\n",
    "        loss_t = validation_loss(test_input, test_output, languageModel, loss_f, 100)\n",
    "#         if loss_t < lbest:\n",
    "#             lbest = loss_t\n",
    "#             bestp = mlp.state_dict()\n",
    "       \n",
    "        print(\"test loss:\",loss_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LanguageModel(\n",
      "  (embedding_layer): Embedding(2362, 16)\n",
      "  (linear1): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (linear2): Linear(in_features=128, out_features=2362, bias=True)\n",
      ")\n",
      "Epoch: 0\n",
      "Split train and test, train size: 39500, test_size: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 5.665371928215027\n",
      "Epoch: 1\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 5.142009539604187\n",
      "Epoch: 2\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 4.755396509170533\n",
      "Epoch: 3\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 4.366890208721161\n",
      "Epoch: 4\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 4.061720848083496\n",
      "Epoch: 5\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 3.8121746039390563\n",
      "Epoch: 6\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 3.607817988395691\n",
      "Epoch: 7\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 3.4171538972854614\n",
      "Epoch: 8\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 3.2289458298683167\n",
      "Epoch: 9\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 3.1340517950057984\n",
      "Epoch: 10\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 3.0282026433944704\n",
      "Epoch: 11\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 2.952309410572052\n",
      "Epoch: 12\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 2.9059852242469786\n",
      "Epoch: 13\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 2.795420515537262\n",
      "Epoch: 14\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 2.7558386850357057\n",
      "Epoch: 15\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 2.673228316307068\n",
      "Epoch: 16\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 2.6234912490844726\n",
      "Epoch: 17\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 2.5725254106521604\n",
      "Epoch: 18\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 2.539563103914261\n",
      "Epoch: 19\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 2.520332703590393\n",
      "Epoch: 20\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 2.4826597476005556\n",
      "Epoch: 21\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 2.439068466424942\n",
      "Epoch: 22\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 2.4161511731147765\n",
      "Epoch: 23\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 2.388419771194458\n",
      "Epoch: 24\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 2.3832037007808684\n",
      "Epoch: 25\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 2.3498401534557343\n",
      "Epoch: 26\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 2.3312528038024904\n",
      "Epoch: 27\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 2.2964989936351774\n",
      "Epoch: 28\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 2.2721023643016816\n",
      "Epoch: 29\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 2.234593861103058\n",
      "Epoch: 30\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 2.2473870158195495\n",
      "Epoch: 31\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 2.1967322254180908\n",
      "Epoch: 32\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 2.1980931723117827\n",
      "Epoch: 33\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 2.187814147472382\n",
      "Epoch: 34\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 2.140650678873062\n",
      "Epoch: 35\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 2.167540158033371\n",
      "Epoch: 36\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 2.127499197721481\n",
      "Epoch: 37\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 2.1332529973983765\n",
      "Epoch: 38\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 2.132889585494995\n",
      "Epoch: 39\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 2.1217903161048888\n",
      "Epoch: 40\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 2.0604318606853487\n",
      "Epoch: 41\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 2.088132253885269\n",
      "Epoch: 42\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 2.0748095059394838\n",
      "Epoch: 43\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 2.0961844336986544\n",
      "Epoch: 44\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 2.0357166218757627\n",
      "Epoch: 45\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 2.048574435710907\n",
      "Epoch: 46\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 2.0081684148311614\n",
      "Epoch: 47\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 2.045466324090958\n",
      "Epoch: 48\n",
      "Split train and test, train size: 39500, test_size: 10000\n",
      "test loss: 1.9996169018745422\n",
      "Epoch: 49\n",
      "Split train and test, train size: 39500, test_size: 10000\n"
     ]
    }
   ],
   "source": [
    "languageModel2 = LanguageModel(my_corpus.vocSize, 16, my_corpus.context_size, False)\n",
    "print(languageModel2)\n",
    "loss_f = F.cross_entropy\n",
    "epoches = 50\n",
    "lr = 0.01\n",
    "optimizer = optim.Adam(languageModel2.parameters(), lr=lr)\n",
    "loss_report = 1000\n",
    "for epoch in range(epoches):\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    train_idxs, test_idxs = split_train_test(my_corpus.data[\"input_tensor\"], my_corpus.data[\"output_tensor\"], 39500, 10000)\n",
    "    train_input = my_corpus.data[\"input_tensor\"][train_idxs]\n",
    "    train_output = my_corpus.data[\"output_tensor\"][train_idxs]\n",
    "    test_input = my_corpus.data[\"input_tensor\"][test_idxs]\n",
    "    test_output = my_corpus.data[\"output_tensor\"][test_idxs]\n",
    "    for i, (input_b, out_b) in enumerate(get_batches(train_input, train_output, 100)):\n",
    "        input_b = torch.argmax(to_var(input_b), dim=2).type(torch.LongTensor)\n",
    "        out_b = torch.argmax(to_var(out_b), dim=1).type(torch.LongTensor)\n",
    "        optimizer.zero_grad()   # zero the gradient buffers\n",
    "        output = languageModel2(input_b)\n",
    "        loss = loss_f(output, out_b)\n",
    "        loss.backward()\n",
    "        optimizer.step()    #\n",
    "        if (i+1)%loss_report == 0:\n",
    "            print(\"Batch:\", i+1, \"loss:\",loss.item())\n",
    "    # validation error\n",
    "    with torch.no_grad():\n",
    "        loss_t = validation_loss(test_input, test_output, languageModel2, loss_f, 100)\n",
    "#         if loss_t < lbest:\n",
    "#             lbest = loss_t\n",
    "#             bestp = mlp.state_dict()\n",
    "       \n",
    "        print(\"test loss:\",loss_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_next_word(languageModel2, my_corpus, '我对你的')\n",
    "show_next_word(languageModel2, my_corpus, '起已希望')\n",
    "show_next_word(languageModel2, my_corpus, ['STA', '我', '有', '你'])\n",
    "show_next_word(languageModel2, my_corpus, ['STA', '我', '有', 'END'])\n",
    "show_next_word(languageModel2, my_corpus, '就你好吗')\n",
    "show_next_word(languageModel2, my_corpus, '就就就就')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_obj = pickle.load(open('./data.pk', 'rb'))\n",
    "vocab = data_obj['vocab']\n",
    "train_inputs, train_targets = data_obj['train_inputs'], data_obj['train_targets']\n",
    "valid_inputs, valid_targets = data_obj['valid_inputs'], data_obj['valid_targets']\n",
    "test_inputs, test_targets = data_obj['test_inputs'], data_obj['test_targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(372500, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(372500,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LanguageModel(\n",
      "  (embedding_layer): Embedding(250, 16)\n",
      "  (linear1): Linear(in_features=48, out_features=128, bias=True)\n",
      "  (linear2): Linear(in_features=128, out_features=250, bias=True)\n",
      ")\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1000 loss: 4.065740585327148\n",
      "Batch: 2000 loss: 3.846798896789551\n",
      "Batch: 3000 loss: 3.850691795349121\n",
      "test loss: 3.7236439556203864\n",
      "Epoch: 1\n",
      "Batch: 1000 loss: 3.903912305831909\n",
      "Batch: 2000 loss: 3.47682523727417\n",
      "Batch: 3000 loss: 3.4247121810913086\n",
      "test loss: 3.6603608218572474\n",
      "Epoch: 2\n",
      "Batch: 1000 loss: 3.6736810207366943\n",
      "Batch: 2000 loss: 3.6680121421813965\n",
      "Batch: 3000 loss: 4.133667945861816\n",
      "test loss: 3.6497012225530483\n",
      "Epoch: 3\n",
      "Batch: 1000 loss: 3.8373239040374756\n",
      "Batch: 2000 loss: 3.752830743789673\n",
      "Batch: 3000 loss: 3.7197813987731934\n",
      "test loss: 3.657488513249223\n",
      "Epoch: 4\n",
      "Batch: 1000 loss: 3.8906915187835693\n",
      "Batch: 2000 loss: 3.8080813884735107\n",
      "Batch: 3000 loss: 3.625755548477173\n",
      "test loss: 3.6180806816265147\n",
      "Epoch: 5\n",
      "Batch: 1000 loss: 3.8452494144439697\n",
      "Batch: 2000 loss: 3.647465705871582\n",
      "Batch: 3000 loss: 3.6182546615600586\n",
      "test loss: 3.635104233731506\n",
      "Epoch: 6\n",
      "Batch: 1000 loss: 4.3388590812683105\n",
      "Batch: 2000 loss: 3.405104637145996\n",
      "Batch: 3000 loss: 3.182595729827881\n",
      "test loss: 3.6024559749070035\n",
      "Epoch: 7\n",
      "Batch: 1000 loss: 3.7417919635772705\n",
      "Batch: 2000 loss: 3.187980890274048\n",
      "Batch: 3000 loss: 3.1015536785125732\n",
      "test loss: 3.6129184861336987\n",
      "Epoch: 8\n",
      "Batch: 1000 loss: 3.666123151779175\n",
      "Batch: 2000 loss: 3.4258103370666504\n",
      "Batch: 3000 loss: 3.588360548019409\n",
      "test loss: 3.5790826202720725\n",
      "Epoch: 9\n",
      "Batch: 1000 loss: 3.6255276203155518\n",
      "Batch: 2000 loss: 3.351898193359375\n",
      "Batch: 3000 loss: 3.5600388050079346\n",
      "test loss: 3.6093211025320073\n",
      "Epoch: 10\n",
      "Batch: 1000 loss: 3.7061057090759277\n",
      "Batch: 2000 loss: 3.8525619506835938\n",
      "Batch: 3000 loss: 3.600338935852051\n",
      "test loss: 3.6233076926200622\n",
      "Epoch: 11\n",
      "Batch: 1000 loss: 3.992419719696045\n",
      "Batch: 2000 loss: 3.353231191635132\n",
      "Batch: 3000 loss: 3.5695509910583496\n",
      "test loss: 3.670554805058305\n",
      "Epoch: 12\n",
      "Batch: 1000 loss: 3.7210888862609863\n",
      "Batch: 2000 loss: 3.7555575370788574\n",
      "Batch: 3000 loss: 3.659153699874878\n",
      "test loss: 3.64836775102923\n",
      "Epoch: 13\n",
      "Batch: 1000 loss: 3.846221923828125\n",
      "Batch: 2000 loss: 3.519188642501831\n",
      "Batch: 3000 loss: 3.4073801040649414\n",
      "test loss: 3.5952599653633692\n",
      "Epoch: 14\n",
      "Batch: 1000 loss: 3.5154662132263184\n",
      "Batch: 2000 loss: 3.3805654048919678\n",
      "Batch: 3000 loss: 3.671123743057251\n",
      "test loss: 3.6172885556374825\n",
      "Epoch: 15\n",
      "Batch: 1000 loss: 3.9611928462982178\n",
      "Batch: 2000 loss: 3.7585790157318115\n",
      "Batch: 3000 loss: 3.5829849243164062\n",
      "test loss: 3.597662709861673\n",
      "Epoch: 16\n",
      "Batch: 1000 loss: 3.7382495403289795\n",
      "Batch: 2000 loss: 3.422786235809326\n",
      "Batch: 3000 loss: 3.589109182357788\n",
      "test loss: 3.6057081689116774\n",
      "Epoch: 17\n",
      "Batch: 1000 loss: 3.870035409927368\n",
      "Batch: 2000 loss: 3.214216709136963\n",
      "Batch: 3000 loss: 3.560372829437256\n",
      "test loss: 3.6184746911448817\n",
      "Epoch: 18\n",
      "Batch: 1000 loss: 3.4339075088500977\n",
      "Batch: 2000 loss: 3.4348304271698\n",
      "Batch: 3000 loss: 3.3965959548950195\n",
      "test loss: 3.5794026103070986\n",
      "Epoch: 19\n",
      "Batch: 1000 loss: 3.4026377201080322\n",
      "Batch: 2000 loss: 3.6425671577453613\n",
      "Batch: 3000 loss: 3.6643800735473633\n",
      "test loss: 3.601097762712868\n",
      "Epoch: 20\n",
      "Batch: 1000 loss: 3.449570894241333\n",
      "Batch: 2000 loss: 3.583057165145874\n",
      "Batch: 3000 loss: 3.5611112117767334\n",
      "test loss: 3.6282460807472146\n",
      "Epoch: 21\n",
      "Batch: 1000 loss: 3.3511290550231934\n",
      "Batch: 2000 loss: 3.4993479251861572\n",
      "Batch: 3000 loss: 3.6807661056518555\n",
      "test loss: 3.587147842940464\n",
      "Epoch: 22\n",
      "Batch: 1000 loss: 3.5887274742126465\n",
      "Batch: 2000 loss: 3.4415242671966553\n",
      "Batch: 3000 loss: 3.767784833908081\n",
      "test loss: 3.564838149470668\n",
      "Epoch: 23\n",
      "Batch: 1000 loss: 3.549325942993164\n",
      "Batch: 2000 loss: 4.039292335510254\n",
      "Batch: 3000 loss: 3.565152168273926\n",
      "test loss: 3.5872672783431185\n",
      "Epoch: 24\n",
      "Batch: 1000 loss: 3.47636342048645\n",
      "Batch: 2000 loss: 3.7563412189483643\n",
      "Batch: 3000 loss: 3.8957009315490723\n",
      "test loss: 3.618213334647558\n",
      "Epoch: 25\n",
      "Batch: 1000 loss: 3.9325132369995117\n",
      "Batch: 2000 loss: 3.4881153106689453\n",
      "Batch: 3000 loss: 3.6827874183654785\n",
      "test loss: 3.6606888755675286\n",
      "Epoch: 26\n",
      "Batch: 1000 loss: 3.4708070755004883\n",
      "Batch: 2000 loss: 3.5731353759765625\n",
      "Batch: 3000 loss: 3.6102545261383057\n",
      "test loss: 3.6029787919854606\n",
      "Epoch: 27\n",
      "Batch: 1000 loss: 3.790195941925049\n",
      "Batch: 2000 loss: 3.393815517425537\n",
      "Batch: 3000 loss: 3.4211244583129883\n",
      "test loss: 3.602841156272478\n",
      "Epoch: 28\n",
      "Batch: 1000 loss: 3.865229845046997\n",
      "Batch: 2000 loss: 3.119126796722412\n",
      "Batch: 3000 loss: 3.727229356765747\n",
      "test loss: 3.5896685723335513\n",
      "Epoch: 29\n",
      "Batch: 1000 loss: 3.8913779258728027\n",
      "Batch: 2000 loss: 3.604257583618164\n",
      "Batch: 3000 loss: 3.463502883911133\n",
      "test loss: 3.5632526869414956\n",
      "Epoch: 30\n",
      "Batch: 1000 loss: 3.698598861694336\n",
      "Batch: 2000 loss: 3.5680792331695557\n",
      "Batch: 3000 loss: 3.7214980125427246\n",
      "test loss: 3.5698060117742068\n",
      "Epoch: 31\n",
      "Batch: 1000 loss: 3.7888050079345703\n",
      "Batch: 2000 loss: 3.7130258083343506\n",
      "Batch: 3000 loss: 3.839595079421997\n",
      "test loss: 3.578855786785003\n",
      "Epoch: 32\n",
      "Batch: 1000 loss: 3.488978862762451\n",
      "Batch: 2000 loss: 3.594935894012451\n",
      "Batch: 3000 loss: 3.708031415939331\n",
      "test loss: 3.588031961584604\n",
      "Epoch: 33\n",
      "Batch: 1000 loss: 3.6138482093811035\n",
      "Batch: 2000 loss: 3.4832401275634766\n",
      "Batch: 3000 loss: 3.4285285472869873\n",
      "test loss: 3.5598959466462494\n",
      "Epoch: 34\n",
      "Batch: 1000 loss: 3.506864070892334\n",
      "Batch: 2000 loss: 3.4927773475646973\n",
      "Batch: 3000 loss: 3.423630952835083\n",
      "test loss: 3.5666622192628923\n",
      "Epoch: 35\n",
      "Batch: 1000 loss: 3.2022430896759033\n",
      "Batch: 2000 loss: 3.7990732192993164\n",
      "Batch: 3000 loss: 3.5735390186309814\n",
      "test loss: 3.539171587523594\n",
      "Epoch: 36\n",
      "Batch: 1000 loss: 4.153641223907471\n",
      "Batch: 2000 loss: 3.8951663970947266\n",
      "Batch: 3000 loss: 3.4419116973876953\n",
      "test loss: 3.565837634507046\n",
      "Epoch: 37\n",
      "Batch: 1000 loss: 3.2946746349334717\n",
      "Batch: 2000 loss: 3.817521333694458\n",
      "Batch: 3000 loss: 3.4119327068328857\n",
      "test loss: 3.5453928080938195\n",
      "Epoch: 38\n",
      "Batch: 1000 loss: 3.6272096633911133\n",
      "Batch: 2000 loss: 3.5115745067596436\n",
      "Batch: 3000 loss: 3.792588710784912\n",
      "test loss: 3.560462943969234\n",
      "Epoch: 39\n",
      "Batch: 1000 loss: 3.4526455402374268\n",
      "Batch: 2000 loss: 3.858971357345581\n",
      "Batch: 3000 loss: 3.1411612033843994\n",
      "test loss: 3.52126612406905\n",
      "Epoch: 40\n",
      "Batch: 1000 loss: 3.5632550716400146\n",
      "Batch: 2000 loss: 3.850919246673584\n",
      "Batch: 3000 loss: 3.365598678588867\n",
      "test loss: 3.5346718029309345\n",
      "Epoch: 41\n",
      "Batch: 1000 loss: 3.472949743270874\n",
      "Batch: 2000 loss: 3.631631374359131\n",
      "Batch: 3000 loss: 3.4807162284851074\n",
      "test loss: 3.569208151807067\n",
      "Epoch: 42\n",
      "Batch: 1000 loss: 3.4453115463256836\n",
      "Batch: 2000 loss: 3.6967110633850098\n",
      "Batch: 3000 loss: 3.463876724243164\n",
      "test loss: 3.556135404750865\n",
      "Epoch: 43\n",
      "Batch: 1000 loss: 3.5927090644836426\n",
      "Batch: 2000 loss: 3.7645769119262695\n",
      "Batch: 3000 loss: 3.7803902626037598\n",
      "test loss: 3.576517263022802\n",
      "Epoch: 44\n",
      "Batch: 1000 loss: 3.60549259185791\n",
      "Batch: 2000 loss: 3.4838075637817383\n",
      "Batch: 3000 loss: 3.2023532390594482\n",
      "test loss: 3.5862363415379677\n",
      "Epoch: 45\n",
      "Batch: 1000 loss: 3.8019537925720215\n",
      "Batch: 2000 loss: 3.2621397972106934\n",
      "Batch: 3000 loss: 3.9665687084198\n",
      "test loss: 3.573281298401535\n",
      "Epoch: 46\n",
      "Batch: 1000 loss: 3.6124203205108643\n",
      "Batch: 2000 loss: 3.415299654006958\n",
      "Batch: 3000 loss: 3.375441312789917\n",
      "test loss: 3.578142026675645\n",
      "Epoch: 47\n",
      "Batch: 1000 loss: 3.4783315658569336\n",
      "Batch: 2000 loss: 3.0645322799682617\n",
      "Batch: 3000 loss: 3.9044313430786133\n",
      "test loss: 3.5873607102260796\n",
      "Epoch: 48\n",
      "Batch: 1000 loss: 3.747060775756836\n",
      "Batch: 2000 loss: 3.3230276107788086\n",
      "Batch: 3000 loss: 3.8583879470825195\n",
      "test loss: 3.570362819138394\n",
      "Epoch: 49\n",
      "Batch: 1000 loss: 3.8055996894836426\n",
      "Batch: 2000 loss: 3.720255136489868\n",
      "Batch: 3000 loss: 3.0969924926757812\n",
      "test loss: 3.570139031769127\n"
     ]
    }
   ],
   "source": [
    "def test_validation_loss(test_input, test_output, model, loss_func, batch_size=100):\n",
    "    with torch.no_grad():\n",
    "        loss_t = 0.0\n",
    "        for i, (input_b, out_b) in enumerate(get_batches(test_input, test_output, batch_size)):\n",
    "            if model.linear:\n",
    "                input_b = to_var(input_b)\n",
    "            output_predict = model(input_b)\n",
    "            loss_t += loss_func(output_predict, out_b).item()\n",
    "    return loss_t/(i+1)\n",
    "languageModel = LanguageModel(250, 16, 3, False)\n",
    "print(languageModel)\n",
    "loss_f = F.cross_entropy\n",
    "epoches = 50\n",
    "lr = 0.1\n",
    "optimizer = optim.Adam(languageModel.parameters(), lr=lr)\n",
    "loss_report = 1000\n",
    "for epoch in range(epoches):\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    train_input = torch.tensor(train_inputs).type(torch.LongTensor)\n",
    "    train_output = torch.tensor(train_targets).type(torch.LongTensor)\n",
    "    test_input = torch.tensor(valid_inputs).type(torch.LongTensor)\n",
    "    test_output = torch.tensor(valid_targets).type(torch.LongTensor)\n",
    "    for i, (input_b, out_b) in enumerate(get_batches(train_input, train_output, 100)):\n",
    "#         input_b = torch.argmax(to_var(input_b), dim=2).type(torch.LongTensor)\n",
    "#         out_b = torch.argmax(to_var(out_b), dim=1).type(torch.LongTensor)\n",
    "        optimizer.zero_grad()   # zero the gradient buffers\n",
    "        output = languageModel(input_b)\n",
    "        loss = loss_f(output, out_b)\n",
    "        loss.backward()\n",
    "        optimizer.step()    #\n",
    "        if (i+1)%loss_report == 0:\n",
    "            print(\"Batch:\", i+1, \"loss:\",loss.item())\n",
    "    # validation error\n",
    "    with torch.no_grad():\n",
    "        loss_t = test_validation_loss(test_input, test_output, languageModel, loss_f, 100)\n",
    "#         if loss_t < lbest:\n",
    "#             lbest = loss_t\n",
    "#             bestp = mlp.state_dict()\n",
    "       \n",
    "        print(\"test loss:\",loss_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_next_word( word1, word2, word3, languageModel, k=10):\n",
    "        \"\"\"List the top k predictions for the next word along with their probabilities.\n",
    "        Inputs:\n",
    "            word1: The first word as a string.\n",
    "            word2: The second word as a string.\n",
    "            word3: The third word as a string.\n",
    "            k: The k most probable predictions are shown.\n",
    "        Example usage:\n",
    "            model.predict_next_word('john', 'might', 'be', 3)\n",
    "            model.predict_next_word('life', 'in', 'new', 3)\"\"\"\n",
    "            \n",
    "        if word1 not in vocab:\n",
    "            raise RuntimeError('Word \"{}\" not in vocabulary.'.format(word1))\n",
    "        if word2 not in vocab:\n",
    "            raise RuntimeError('Word \"{}\" not in vocabulary.'.format(word2))\n",
    "        if word3 not in vocab:\n",
    "            raise RuntimeError('Word \"{}\" not in vocabulary.'.format(word3))\n",
    "\n",
    "        idx1, idx2, idx3 = vocab.index(word1), vocab.index(word2), vocab.index(word3)\n",
    "        input = torch.tensor(np.array([idx1, idx2, idx3]).reshape((1, -1))).type(torch.LongTensor)\n",
    "        output = languageModel(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "v =predict_next_word(\"game\", \"play\", \"play\", languageModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "j = torch.argmax(v, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
