{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def accuracy( C ):\n",
    "    ''' Compute accuracy given Numpy array confusion matrix C. Returns a floating point value '''\n",
    "    a = 0\n",
    "    for i in range(4):\n",
    "      a += C[i][i]\n",
    "    \n",
    "    return a/np.sum(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get batches for each epoch\n",
    "def get_batches(inputs, targets, batch_size, shuffle=True):\n",
    "    \"\"\"Divide a dataset into mini-batches of a given size. This is a'generator'.\"\"\"\n",
    "    \n",
    "    if inputs.shape[0] % batch_size != 0:\n",
    "        raise RuntimeError('The number of data points must be a multiple of the batch size.')\n",
    "    num_batches = inputs.shape[0] // batch_size\n",
    "\n",
    "    if shuffle:\n",
    "        idxs = np.random.permutation(inputs.shape[0])\n",
    "        inputs = inputs[idxs, :]\n",
    "        targets = targets[idxs]\n",
    "\n",
    "    for m in range(num_batches):\n",
    "        yield inputs[m*batch_size:(m+1)*batch_size, :], \\\n",
    "              targets[m*batch_size:(m+1)*batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_split(inputs, targets, ratio=0.2, shuffle=True):\n",
    "    num_train = int(inputs.shape[0]*(1-ratio))\n",
    "    indices = np.arange(inputs.shape[0])\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    X_train = inputs[indices[:num_train]]\n",
    "    Y_train = targets[indices[:num_train]]\n",
    "    X_test = inputs[indices[num_train:]]\n",
    "    Y_test = targets[indices[num_train:]]\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (h1): Sequential(\n",
      "    (0): Linear(in_features=164, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (h2): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=164, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (o): Sequential(\n",
      "    (0): Linear(in_features=196, out_features=4, bias=True)\n",
      "    (1): Softmax()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.h1 = nn.Sequential(\n",
    "                    nn.Linear(164, 32),\n",
    "                    nn.ReLU())\n",
    "        self.h2 = nn.Sequential(\n",
    "                    nn.Linear(32, 164),\n",
    "                    nn.ReLU())\n",
    "        self.o = nn.Sequential(\n",
    "                    nn.Linear(196, 4),\n",
    "                    nn.Softmax())\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        h1 = self.h1(x)\n",
    "        h2 = self.h2(h1)\n",
    "        y = self.o(torch.cat((h2,h1), 1))\n",
    "        return y\n",
    "\n",
    "\n",
    "mlp = MLP()\n",
    "print(mlp)\n",
    "loss_f = F.cross_entropy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and labels\n",
    "with open(\"extracted/dataSmaller.pk\", \"rb\") as f:\n",
    "    X, Y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = train_test_split(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((80000, 164), (80000,), (20000, 164), (20000,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "X_train = Variable(torch.from_numpy(X_train).float())\n",
    "Y_train = Variable(torch.from_numpy(Y_train).long())\n",
    "X_test = Variable(torch.from_numpy(X_test).float())\n",
    "Y_test = Variable(torch.from_numpy(Y_test).long())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 200 loss: 1.2624366283416748\n",
      "Batch: 400 loss: 1.2252612113952637\n",
      "Batch: 600 loss: 1.3314286470413208\n",
      "Batch: 800 loss: 1.215613842010498\n",
      "test loss: 1.3210723400115967 acc : 0.3899\n",
      "Epoch: 2\n",
      "Batch: 200 loss: 1.257117748260498\n",
      "Batch: 400 loss: 1.2614922523498535\n",
      "Batch: 600 loss: 1.3160804510116577\n",
      "Batch: 800 loss: 1.2621575593948364\n",
      "test loss: 1.3223047256469727 acc : 0.3871\n",
      "Epoch: 3\n",
      "Batch: 200 loss: 1.2208399772644043\n",
      "Batch: 400 loss: 1.3214794397354126\n",
      "Batch: 600 loss: 1.2622885704040527\n",
      "Batch: 800 loss: 1.222031593322754\n",
      "test loss: 1.3224413394927979 acc : 0.38805\n",
      "Epoch: 4\n",
      "Batch: 200 loss: 1.2287784814834595\n",
      "Batch: 400 loss: 1.2196550369262695\n",
      "Batch: 600 loss: 1.242885947227478\n",
      "Batch: 800 loss: 1.2609449625015259\n",
      "test loss: 1.321339726448059 acc : 0.3884\n",
      "Epoch: 5\n",
      "Batch: 200 loss: 1.2397960424423218\n",
      "Batch: 400 loss: 1.191847562789917\n",
      "Batch: 600 loss: 1.3163020610809326\n",
      "Batch: 800 loss: 1.266919493675232\n",
      "test loss: 1.3229745626449585 acc : 0.38855\n",
      "Epoch: 6\n",
      "Batch: 200 loss: 1.213465690612793\n",
      "Batch: 400 loss: 1.3562411069869995\n",
      "Batch: 600 loss: 1.2707935571670532\n",
      "Batch: 800 loss: 1.3102595806121826\n",
      "test loss: 1.3217353820800781 acc : 0.39015\n",
      "Epoch: 7\n",
      "Batch: 200 loss: 1.3246058225631714\n",
      "Batch: 400 loss: 1.2403037548065186\n",
      "Batch: 600 loss: 1.2083847522735596\n",
      "Batch: 800 loss: 1.3361071348190308\n",
      "test loss: 1.3219257593154907 acc : 0.3886\n",
      "Epoch: 8\n",
      "Batch: 200 loss: 1.2136350870132446\n",
      "Batch: 400 loss: 1.2309643030166626\n",
      "Batch: 600 loss: 1.232511043548584\n",
      "Batch: 800 loss: 1.3439292907714844\n",
      "test loss: 1.321915626525879 acc : 0.38725\n",
      "Epoch: 9\n",
      "Batch: 200 loss: 1.3303486108779907\n",
      "Batch: 400 loss: 1.3139969110488892\n",
      "Batch: 600 loss: 1.2426530122756958\n",
      "Batch: 800 loss: 1.311344861984253\n",
      "test loss: 1.32245934009552 acc : 0.3896\n",
      "Epoch: 10\n",
      "Batch: 200 loss: 1.2799742221832275\n",
      "Batch: 400 loss: 1.2910265922546387\n",
      "Batch: 600 loss: 1.3298399448394775\n",
      "Batch: 800 loss: 1.1936101913452148\n",
      "test loss: 1.321815013885498 acc : 0.39025\n"
     ]
    }
   ],
   "source": [
    "epoches = 10\n",
    "lr = 0.0005\n",
    "optimizer = optim.Adam(mlp.parameters(), lr=lr)\n",
    "batch_size = 100\n",
    "loss_report = 200\n",
    "\n",
    "for epoch in range(epoches):\n",
    "    print(\"Epoch:\", epoch+1)\n",
    "    for i, (input_batch, target_batch) in enumerate(get_batches(X_train, Y_train, batch_size)):\n",
    "        optimizer.zero_grad()   # zero the gradient buffers\n",
    "        output = mlp(input_batch)\n",
    "        loss = loss_f(output, target_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()    # Does the update\n",
    "        if (i+1)%loss_report == 0:\n",
    "            print(\"Batch:\", i+1, \"loss:\",loss.item())\n",
    "    # validation error\n",
    "    with torch.no_grad():\n",
    "        y_predict = mlp(X_test)\n",
    "        loss_t = loss_f(y_predict, Y_test)\n",
    "#         if loss_t < lbest:\n",
    "#             lbest = loss_t\n",
    "#             bestp = mlp.state_dict()\n",
    "        y_p = torch.argmax(y_predict, dim = 1).numpy()\n",
    "        c2 = confusion_matrix(Y_test, y_p)\n",
    "        print(\"test loss:\",loss_t.item(), \"acc :\", accuracy(c2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
